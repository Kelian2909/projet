{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c1d3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from utils import data_string_to_float\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca6a46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory where individual html files are stored\n",
    "statspath = \"intraQuarter/_KeyStats/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9aadb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of features to parse from the html files\n",
    "features = [  # Valuation measures\n",
    "    \"Market Cap\",\n",
    "    \"Enterprise Value\",\n",
    "    \"Trailing P/E\",\n",
    "    \"Forward P/E\",\n",
    "    \"PEG Ratio\",\n",
    "    \"Price/Sales\",\n",
    "    \"Price/Book\",\n",
    "    \"Enterprise Value/Revenue\",\n",
    "    \"Enterprise Value/EBITDA\",\n",
    "    #  Financial highlights\n",
    "    \"Profit Margin\",\n",
    "    \"Operating Margin\",\n",
    "    \"Return on Assets\",\n",
    "    \"Return on Equity\",\n",
    "    \"Revenue\",\n",
    "    \"Revenue Per Share\",\n",
    "    \"Qtrly Revenue Growth\",\n",
    "    \"Gross Profit\",\n",
    "    \"EBITDA\",\n",
    "    \"Net Income Avl to Common\",\n",
    "    \"Diluted EPS\",\n",
    "    \"Qtrly Earnings Growth\",\n",
    "    \"Total Cash\",\n",
    "    \"Total Cash Per Share\",\n",
    "    \"Total Debt\",\n",
    "    \"Total Debt/Equity\",\n",
    "    \"Current Ratio\",\n",
    "    \"Book Value Per Share\",\n",
    "    \"Operating Cash Flow\",\n",
    "    \"Levered Free Cash Flow\",\n",
    "    # Trading information\n",
    "    \"Beta\",\n",
    "    \"50-Day Moving Average\",\n",
    "    \"200-Day Moving Average\",\n",
    "    \"Avg Vol (3 month)\",\n",
    "    \"Shares Outstanding\",\n",
    "    \"Float\",\n",
    "    \"% Held by Insiders\",\n",
    "    \"% Held by Institutions\",\n",
    "    \"Shares Short (as of\",\n",
    "    \"Short Ratio\",\n",
    "    \"Short % of Float\",\n",
    "    \"Shares Short (prior month\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21004e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_price_data():\n",
    "    \"\"\"\n",
    "    Currently, the sp500 and stock price datasets we downloaded do not have any data for\n",
    "    days when the market was closed (weekends and public holidays). We need to amend this so that\n",
    "    all rows are included. Doing this now saves a lot of effort when we actually create the\n",
    "    keystats dataset, which requires that we have stock data every day.\n",
    "    :return: SP500 and stock dataframes, with no missing rows.\n",
    "    \"\"\"\n",
    "    # Read in SP500 data and stock data, parsing the dates.\n",
    "    sp500_raw_data = pd.read_csv(\"sp500_index.csv\", index_col=\"Date\", parse_dates=True)\n",
    "    stock_raw_data = pd.read_csv(\"stock_prices.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "    # We will reindex to include the weekends.\n",
    "    start_date = str(stock_raw_data.index[0])\n",
    "    end_date = str(stock_raw_data.index[-1])\n",
    "    idx = pd.date_range(start_date, end_date)\n",
    "    sp500_raw_data = sp500_raw_data.reindex(idx)\n",
    "    stock_raw_data = stock_raw_data.reindex(idx)\n",
    "\n",
    "    # Now the weekends are NaN, so we fill forward these NaNs\n",
    "    # (i.e weekends take the value of Friday's adjusted close).\n",
    "    sp500_raw_data.ffill(inplace=True)\n",
    "    stock_raw_data.ffill(inplace=True)\n",
    "\n",
    "    return sp500_raw_data, stock_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "474b27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_keystats(sp500_df, stock_df):\n",
    "    \"\"\"\n",
    "    We have downloaded a large number of html files, which are snapshots of a ticker at different times,\n",
    "    containing the fundamental data (our features). To extract the key statistics, we use regex.\n",
    "    For supervised machine learning, we also need the data that will form our dependent variable,\n",
    "    the performance of the stock compared to the SP500.\n",
    "    :sp500_df: dataframe containing SP500 prices\n",
    "    :stock_df: dataframe containing stock prices\n",
    "    :return: a dataframe of training data (i.e features and the components of our dependent variable)\n",
    "    \"\"\"\n",
    "    # The tickers whose data is to be parsed.\n",
    "    stock_list = [x[0] for x in os.walk(statspath)]\n",
    "    stock_list = stock_list[1:]\n",
    "\n",
    "    # Creating a new dataframe which we will later fill.\n",
    "    df_columns = [\n",
    "        \"Date\",\n",
    "        \"Unix\",\n",
    "        \"Ticker\",\n",
    "        \"Price\",\n",
    "        \"stock_p_change\",\n",
    "        \"SP500\",\n",
    "        \"SP500_p_change\",\n",
    "    ] + features\n",
    "\n",
    "    df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "    # tqdm is a simple progress bar\n",
    "    for stock_directory in tqdm(stock_list, desc=\"Parsing progress:\", unit=\"tickers\"):\n",
    "        keystats_html_files = os.listdir(stock_directory)\n",
    "\n",
    "        # Snippet to get rid of the .DS_Store file in macOS\n",
    "        if \".DS_Store\" in keystats_html_files:\n",
    "            keystats_html_files.remove(\".DS_Store\")\n",
    "\n",
    "        ticker = stock_directory.split(statspath)[1]\n",
    "\n",
    "        for file in keystats_html_files:\n",
    "            # Convert the datetime format of our file to unix time\n",
    "            date_stamp = datetime.strptime(file, \"%Y%m%d%H%M%S.html\")\n",
    "            unix_time = time.mktime(date_stamp.timetuple())\n",
    "\n",
    "            # Read in the html file as a string.\n",
    "            full_file_path = stock_directory + \"/\" + file\n",
    "\n",
    "            # This will store the parsed values\n",
    "            value_list = []\n",
    "\n",
    "            with open(full_file_path, \"r\") as source:\n",
    "                source = source.read()\n",
    "                # Remove commas from the html to make parsing easier.\n",
    "                source = source.replace(\",\", \"\")\n",
    "\n",
    "                # Regex search for the different variables in the html file, then append to value_list\n",
    "                for variable in features:\n",
    "                    # Search for the table entry adjacent to the variable name.\n",
    "                    try:\n",
    "                        regex = (\n",
    "                            r\">\"\n",
    "                            + re.escape(variable)\n",
    "                            + r\".*?(\\-?\\d+\\.*\\d*K?M?B?|N/A[\\\\n|\\s]*|>0|NaN)%?\"\n",
    "                            r\"(</td>|</span>)\"\n",
    "                        )\n",
    "                        value = re.search(regex, source, flags=re.DOTALL).group(1)\n",
    "\n",
    "                        # Dealing with number formatting\n",
    "                        value_list.append(data_string_to_float(value))\n",
    "\n",
    "                    # The data may not be present. Process accordingly\n",
    "                    except AttributeError:\n",
    "                        # In the past, 'Avg Vol' was instead named 'Average Volume'\n",
    "                        # If 'Avg Vol' fails, search for 'Average Volume'.\n",
    "                        if variable == \"Avg Vol (3 month)\":\n",
    "                            try:\n",
    "                                new_variable = \">Average Volume (3 month)\"\n",
    "                                regex = (\n",
    "                                    re.escape(new_variable)\n",
    "                                    + r\".*?(\\-?\\d+\\.*\\d*K?M?B?|N/A[\\\\n|\\s]*|>0)%?\"\n",
    "                                    r\"(</td>|</span>)\"\n",
    "                                )\n",
    "                                value = re.search(regex, source, flags=re.DOTALL).group(\n",
    "                                    1\n",
    "                                )\n",
    "                                value_list.append(data_string_to_float(value))\n",
    "                            except AttributeError:\n",
    "                                value_list.append(\"N/A\")\n",
    "                        else:\n",
    "                            value_list.append(\"N/A\")\n",
    "\n",
    "            # We need the stock price and SP500 price now and one year from now.\n",
    "            # Convert from unix time to YYYY-MM-DD, so we can look for the price in the dataframe\n",
    "            # then calculate the percentage change.\n",
    "            current_date = datetime.fromtimestamp(unix_time).strftime(\"%Y-%m-%d\")\n",
    "            one_year_later = datetime.fromtimestamp(unix_time + 31536000).strftime(\n",
    "                \"%Y-%m-%d\"\n",
    "            )\n",
    "\n",
    "            # SP500 prices now and one year later, and the percentage change\n",
    "            sp500_price = float(sp500_df.loc[current_date, \"Close\"])\n",
    "            sp500_1y_price = float(sp500_df.loc[one_year_later, \"Close\"])\n",
    "\n",
    "            sp500_p_change = round(\n",
    "                ((sp500_1y_price - sp500_price) / sp500_price * 100), 2\n",
    "            )\n",
    "\n",
    "            # Stock prices now and one year later. We need a try/except because some data is missing\n",
    "            stock_price, stock_1y_price = \"N/A\", \"N/A\"\n",
    "            try:\n",
    "                stock_price = float(stock_df.loc[current_date, ticker.upper()])\n",
    "                stock_1y_price = float(stock_df.loc[one_year_later, ticker.upper()])\n",
    "            except KeyError:\n",
    "                # If stock data is missing, we must skip this datapoint\n",
    "                # print(f\"PRICE RETRIEVAL ERROR for {ticker}\")\n",
    "                continue\n",
    "\n",
    "            stock_p_change = round(\n",
    "                ((stock_1y_price - stock_price) / stock_price * 100), 2\n",
    "            )\n",
    "\n",
    "            # Append all our data to the dataframe.\n",
    "            new_df_row = [\n",
    "                date_stamp,\n",
    "                unix_time,\n",
    "                ticker,\n",
    "                stock_price,\n",
    "                stock_p_change,\n",
    "                sp500_price,\n",
    "                sp500_p_change,\n",
    "            ] + value_list\n",
    "\n",
    "            new_row = pd.DataFrame([dict(zip(df_columns, new_df_row))])\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Remove rows with missing stock price data\n",
    "    # Supprimer les lignes qui n'ont même pas le prix du stock ou l'évolution\n",
    "    df = df.dropna(subset=[\"Price\", \"stock_p_change\"])\n",
    "\n",
    "    # Optionnel : supprimer les lignes trop incomplètes (par exemple moins de 70% de données)\n",
    "    df = df.dropna(thresh=int(0.7 * df.shape[1]))\n",
    "    print(f\"{len(df)} lignes finales prêtes pour l'entraînement.\")\n",
    "\n",
    "    # Output the CSV\n",
    "    df.to_csv(\"keystats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3a98ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_keystats(sp500_df, stock_df):\n",
    "    stock_list = [x[0] for x in os.walk(statspath)][1:]\n",
    "\n",
    "    df_columns = [\n",
    "        \"Date\", \"Unix\", \"Ticker\", \"Price\", \"stock_p_change\", \"SP500\", \"SP500_p_change\"\n",
    "    ] + features\n",
    "\n",
    "    df = pd.DataFrame(columns=df_columns)\n",
    "    ligne_count = 0\n",
    "\n",
    "    for stock_directory in tqdm(stock_list, desc=\"Parsing progress:\", unit=\"tickers\"):\n",
    "        ticker = stock_directory.split(statspath)[1]\n",
    "        files = os.listdir(stock_directory)\n",
    "        if \".DS_Store\" in files:\n",
    "            files.remove(\".DS_Store\")\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                date_stamp = datetime.strptime(file, \"%Y%m%d%H%M%S.html\")\n",
    "                unix_time = time.mktime(date_stamp.timetuple())\n",
    "                current_date = datetime.fromtimestamp(unix_time).strftime(\"%Y-%m-%d\")\n",
    "                one_year_later = datetime.fromtimestamp(unix_time + 31536000).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                if current_date not in sp500_df.index or one_year_later not in sp500_df.index:\n",
    "                    print(f\"❌ SP500 : date absente - {current_date} ou {one_year_later}\")\n",
    "                    continue\n",
    "                if ticker.upper() not in stock_df.columns:\n",
    "                    print(f\"❌ Ticker {ticker.upper()} non présent dans stock_df\")\n",
    "                    continue\n",
    "\n",
    "                full_file_path = os.path.join(stock_directory, file)\n",
    "                with open(full_file_path, \"r\") as f:\n",
    "                    source = f.read().replace(\",\", \"\")\n",
    "\n",
    "                value_list = []\n",
    "                for variable in features:\n",
    "                    try:\n",
    "                        regex = (\n",
    "                            r\">\" + re.escape(variable) +\n",
    "                            r\".*?(\\-?\\d+\\.*\\d*K?M?B?|N/A[\\n|\\s]*|>0|NaN)%?\" +\n",
    "                            r\"(</td>|</span>)\"\n",
    "                        )\n",
    "                        value = re.search(regex, source, flags=re.DOTALL).group(1)\n",
    "                        value_list.append(data_string_to_float(value))\n",
    "                    except AttributeError:\n",
    "                        if variable == \"Avg Vol (3 month)\":\n",
    "                            try:\n",
    "                                regex = (\n",
    "                                    r\">Average Volume \\(3 month\\).*?(\\-?\\d+\\.*\\d*K?M?B?|N/A[\\n|\\s]*|>0)%?\" +\n",
    "                                    r\"(</td>|</span>)\"\n",
    "                                )\n",
    "                                value = re.search(regex, source, flags=re.DOTALL).group(1)\n",
    "                                value_list.append(data_string_to_float(value))\n",
    "                            except AttributeError:\n",
    "                                value_list.append(\"N/A\")\n",
    "                        else:\n",
    "                            value_list.append(\"N/A\")\n",
    "\n",
    "                # Debug des variables extraites\n",
    "                nb_na = value_list.count(\"N/A\")\n",
    "                print(f\"[{ticker} - {current_date}] ➤ {len(value_list) - nb_na}/{len(value_list)} variables valides\")\n",
    "\n",
    "                # Extraire les prix\n",
    "                sp500_price = float(sp500_df.loc[current_date, \"Close\"])\n",
    "                sp500_1y_price = float(sp500_df.loc[one_year_later, \"Close\"])\n",
    "                sp500_p_change = round((sp500_1y_price - sp500_price) / sp500_price * 100, 2)\n",
    "\n",
    "                stock_price = float(stock_df.loc[current_date, ticker.upper()])\n",
    "                stock_1y_price = float(stock_df.loc[one_year_later, ticker.upper()])\n",
    "                stock_p_change = round((stock_1y_price - stock_price) / stock_price * 100, 2)\n",
    "\n",
    "                print(f\"{ticker}: {stock_price} → {stock_1y_price} | SP500: {sp500_price} → {sp500_1y_price}\")\n",
    "\n",
    "                # Ligne à ajouter\n",
    "                new_df_row = [\n",
    "                    date_stamp, unix_time, ticker, stock_price, stock_p_change,\n",
    "                    sp500_price, sp500_p_change\n",
    "                ] + value_list\n",
    "\n",
    "                new_row = pd.DataFrame([dict(zip(df_columns, new_df_row))])\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "                ligne_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur {ticker} / {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Nettoyage minimal\n",
    "    df = df.dropna(subset=[\"Price\", \"stock_p_change\"])\n",
    "    df = df.dropna(thresh=int(0.7 * df.shape[1]))\n",
    "\n",
    "    print(f\"\\n✅ Total lignes collectées : {ligne_count}\")\n",
    "    print(f\"✅ Total lignes finales après nettoyage : {len(df)}\")\n",
    "\n",
    "    df.to_csv(\"keystats.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd3d34b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing progress:: 100%|██████████| 2/2 [00:00<00:00, 2190.24tickers/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total lignes collectées : 0\n",
      "✅ Total lignes finales après nettoyage : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sp500_df, stock_df = preprocess_price_data()\n",
    "    parse_keystats(sp500_df, stock_df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
